import os
import requests
import datetime
import time
import tiktoken  # í† í° ìˆ˜ ê³„ì‚°ì„ ìœ„í•´ tiktoken ì‚¬ìš©
from math import ceil
from langdetect import detect
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from bs4 import BeautifulSoup
import openai
from googleapiclient.discovery import build
from langchain.vectorstores import Chroma
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import VECTOR_DB_PATH, SUMMARY_DIR

# -------------------
# 0. í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ì„¤ì •
# -------------------

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()  # ê¸°ë³¸ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹… ì¶œë ¥
print("OPENAI_API_KEY2:", os.getenv("OPENAI_API_KEY2"))

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY2")

# êµ¬ê¸€ API í‚¤ ì„¤ì •
GEOCODING_API_KEY = os.getenv("GOOGLE_GEOCODING_API_KEY")
GOOGLE_PLACES_API_KEY = os.getenv("GOOGLE_PLACES_API_KEY")

# ì‚¬ìš©í•  ìƒìˆ˜ë“¤
MAX_URLS = 5  # ìµœëŒ€ URL ê°œìˆ˜
CHUNK_SIZE = 2048  # ê° í…ìŠ¤íŠ¸ ì²­í¬ì˜ ìµœëŒ€ í† í° ìˆ˜ (ì¡°ì • ê°€ëŠ¥)
MODEL = "gpt-4o-mini"  # ì‚¬ìš©í•  OpenAI ëª¨ë¸
FINAL_SUMMARY_MAX_TOKENS = 1500  # ìµœì¢… ìš”ì•½ì˜ ìµœëŒ€ í† í° ìˆ˜

# ë²¡í„° DB ê²½ë¡œ
MAIN_DB_PATH = os.getenv("MAIN_DB_PATH", "main_db")

# -------------------
# 1. ë©”ì¸ ì‹¤í–‰ íë¦„
# -------------------
def process_urls(urls):
    start_time = time.time()
    all_text = ""
    video_infos = []
    
    # (1) ì…ë ¥ë°›ì€ URLì„ ìˆœíšŒí•˜ë©´ì„œ í…ìŠ¤íŠ¸/ìë§‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    for idx, url in enumerate(urls, 1):
        print(f"\nURL {idx}/{len(urls)} ì²˜ë¦¬ ì¤‘: {url}")
        try:
            # 1-1) ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            video_title, channel_name = get_video_info(url)
            if video_title and channel_name:
                video_infos.append({
                    'url': url,
                    'title': video_title,
                    'channel': channel_name
                })
            
            # 1-2) URLì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.
            text = process_link(url)
            all_text += f"\n\n--- URL {idx} ë‚´ìš© ---\n{text}"
            print(f"URL {idx} ì²˜ë¦¬ ì™„ë£Œ.")
        except Exception as e:
            print(f"URL {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # (2) ëª¨ë“  URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì œëŒ€ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆë‹¤ë©´ ì—ëŸ¬ ë°œìƒ
    if not all_text.strip():
        raise ValueError("ëª¨ë“  URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # (3) ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ CHUNK_SIZEì— ë§ê²Œ ë¶„í• í•©ë‹ˆë‹¤.
    print("\ní…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
    transcript_chunks = split_text(all_text)
    print(f"í…ìŠ¤íŠ¸ê°€ {len(transcript_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # (4) ë¶„í• ëœ ì²­í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥(ë””ë²„ê¹…/ê²€ì¦ ìš©ë„)
    save_chunks(transcript_chunks)
    
    # (5) ë‚˜ëˆ ì§„ ì²­í¬ë“¤ì„ ìš”ì•½í•©ë‹ˆë‹¤.
    print("\nìš”ì•½ì„ ìƒì„± ì¤‘...")
    final_summary = summarize_text(transcript_chunks)
    
    # (6) ìš”ì•½ì—ì„œ ë°©ë¬¸ ì¥ì†Œëª…ì„ ì¶”ì¶œí•˜ê³ , ì¶”ê°€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    print("\nì¥ì†Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    place_details = []
    place_names = extract_place_names(final_summary)
    
    for place_name in place_names:
        print(f"\n{place_name} ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        details = {}
        
        # (6-1) Google Places APIë¡œ ì¥ì†Œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        try:
            google_details = search_place_details(place_name)
            if google_details:
                details.update(google_details)
                
                # (6-2) ê°€ì ¸ì˜¨ place_nameìœ¼ë¡œ ì‚¬ì§„ URLë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
                photo_url = get_place_photo_google(place_name, GOOGLE_PLACES_API_KEY)
                if photo_url and photo_url != "ì‚¬ì§„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." and photo_url != "API ìš”ì²­ ì‹¤íŒ¨.":
                    details['photos'] = [{
                        'url': photo_url,
                        'title': f'{place_name} ì‚¬ì§„',
                        'description': f'{place_name}ì˜ Google Places APIë¥¼ í†µí•´ ê°€ì ¸ì˜¨ ì‚¬ì§„ì…ë‹ˆë‹¤.'
                    }]
        except Exception as e:
            print(f"Google Places API ì˜¤ë¥˜: {e}")
        
        if details:
            place_details.append(details)
    
    # (7) ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    end_time = time.time()
    processing_time = end_time - start_time
    
    # (8) ìµœì¢… ê²°ê³¼ë¥¼ ë¬¸ìì—´ í˜•íƒœë¡œ êµ¬ì„±
    final_result = f"""
=== ì—¬í–‰ ì •ë³´ ìš”ì•½ ===
ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ

ë¶„ì„í•œ ì˜ìƒ:
{'='*50}"""
    
    # 8-1) ìˆ˜ì§‘ëœ ìœ íŠœë¸Œ ì˜ìƒ ì •ë³´ ì¶œë ¥ìš©
    if video_infos:
        for info in video_infos:
            final_result += f"""
ì œëª©: {info['title']}
ì±„ë„: {info['channel']}
URL: {info['url']}"""
    else:
        final_result += f"""
URL: {chr(10).join(urls)}"""

    final_result += f"\n{'='*50}\n"

    # (9) ì¥ì†Œë³„ ì •ë³´ í†µí•©(ìœ íŠœë¸Œ ìš”ì•½ë‚´ìš© + êµ¬ê¸€ ì •ë³´)
    places_info = {}
    for line in final_summary.split('\n'):
        # ë°©ë¬¸í•œ ì¥ì†Œ íŒŒì‹±
        if line.startswith('ë°©ë¬¸í•œ ì¥ì†Œ:'):
            place_name = line.split('(')[0].replace('ë°©ë¬¸í•œ ì¥ì†Œ:', '').strip()
            if place_name not in places_info:
                places_info[place_name] = {'youtuber_info': [], 'google_info': None}
            
            start_idx = final_summary.find(line)
            end_idx = final_summary.find("\n\në°©ë¬¸í•œ ì¥ì†Œ:", start_idx)
            if end_idx == -1:
                end_idx = len(final_summary)
            place_section = final_summary[start_idx:end_idx]
            
            places_info[place_name]['youtuber_info'] = place_section.split('\n')

    # (10) Google Places API ì •ë³´ì™€ ë§¤ì¹­
    for place in place_details:
        place_name = place.get('name')
        if place_name in places_info:
            places_info[place_name]['google_info'] = place

    # (11) ì¥ì†Œë³„ ìƒì„¸ ì •ë³´ ë¬¸ìì—´ì— ì¶”ê°€
    final_result += "\n=== ì¥ì†Œë³„ ìƒì„¸ ì •ë³´ ===\n"
    
    for idx, (place_name, info) in enumerate(places_info.items(), 1):
        final_result += f"\n{idx}. {place_name}\n{'='*50}\n"
        
        # 11-1) ìœ íŠœë²„ ì •ë³´
        if info['youtuber_info']:
            final_result += "\n[ìœ íŠœë²„ì˜ ë¦¬ë·°]\n"
            
            place_desc = ""
            foods = []
            precautions = []
            recommendations = []
            
            for line in info['youtuber_info']:
                line = line.strip()
                if not line or line.startswith('ë°©ë¬¸í•œ ì¥ì†Œ:'):
                    continue
                    
                if line.startswith('- ì¥ì†Œì„¤ëª…:'):
                    place_desc = line
                elif line.startswith('- ë¨¹ì€ ìŒì‹:'):
                    foods.append(line)
                elif line.startswith('\t- ì„¤ëª…:') and foods:
                    foods[-1] += f"\n{line}"
                elif line.startswith('- ìœ ì˜ ì‚¬í•­:'):
                    precautions.append(line)
                elif line.startswith('\t- ì„¤ëª…:') and precautions:
                    precautions[-1] += f"\n{line}"
                elif line.startswith('- ì¶”ì²œ ì‚¬í•­:'):
                    recommendations.append(line)
                elif line.startswith('\t- ì„¤ëª…:') and recommendations:
                    recommendations[-1] += f"\n{line}"
            
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¶œë ¥
            if place_desc:
                final_result += f"{place_desc}\n"
            
            if foods:
                final_result += "\n[ë¨¹ì€ ìŒì‹]\n"
                for food in foods:
                    final_result += f"{food}\n"
            
            if precautions:
                final_result += "\n[ìœ ì˜ ì‚¬í•­]\n"
                for precaution in precautions:
                    final_result += f"{precaution}\n"
            
            if recommendations:
                final_result += "\n[ì¶”ì²œ ì‚¬í•­]\n"
                for recommendation in recommendations:
                    final_result += f"{recommendation}\n"
        
        # 11-2) êµ¬ê¸€ ì •ë³´
        if info['google_info']:
            google_info = info['google_info']
            opening_hours = google_info.get('opening_hours', ['ì •ë³´ ì—†ìŒ'])
            if not isinstance(opening_hours, list):
                opening_hours = ['ì •ë³´ ì—†ìŒ']

            final_result += f"""
        [êµ¬ê¸€ ì¥ì†Œ ì •ë³´]
        ğŸ  ì£¼ì†Œ: {google_info.get('formatted_address', 'ì •ë³´ ì—†ìŒ')}
        â­ í‰ì : {google_info.get('rating', 'ì •ë³´ ì—†ìŒ')}
        ğŸ“ ì „í™”: {google_info.get('phone', 'ì •ë³´ ì—†ìŒ')}
        ğŸŒ ì›¹ì‚¬ì´íŠ¸: {google_info.get('website', 'ì •ë³´ ì—†ìŒ')}
        ğŸ’° ê°€ê²©ëŒ€: {'â‚©' * google_info.get('price_level', 0) if google_info.get('price_level') else 'ì •ë³´ ì—†ìŒ'}
        â° ì˜ì—…ì‹œê°„:
        {chr(10).join(opening_hours)}

        [ì‚¬ì§„ ë° ë¦¬ë·°]"""
                    
            if 'photos' in google_info and google_info['photos']:
                for photo_idx, photo in enumerate(google_info['photos'], 1):
                    final_result += f"""
                ğŸ“¸ ì‚¬ì§„ {photo_idx}: {photo['url']}
                â­ ë² ìŠ¤íŠ¸ ë¦¬ë·°: {google_info.get('best_review', {}).get('text', 'ë¦¬ë·° ì—†ìŒ') if google_info.get('best_review') else 'ë¦¬ë·° ì—†ìŒ'}"""
            else:
                final_result += "\nì‚¬ì§„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        final_result += f"\n{'='*50}"
    
    # (12) ìµœì¢… ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
    save_final_summary(final_result)

    # ë²¡í„° DBì— ì €ì¥
    try:
        print("\në²¡í„° DBì— ì €ì¥ ì‹œë„ ì¤‘...")
        embeddings = OpenAIEmbeddings()
        
        # config.pyì—ì„œ ì •ì˜ëœ í†µí•© ê²½ë¡œ ì‚¬ìš©
        vectordb = Chroma(
            persist_directory=VECTOR_DB_PATH,  # /Users/minkyeomkim/Desktop/MTL-FE/src/ai_vector/vector_dbs/main_vectordb
            embedding_function=embeddings
        )
        
        metadata = {"url": urls[0] if urls else ""}
        doc = Document(
            page_content=final_result,
            metadata=metadata
        )
        
        vectordb.add_documents([doc])
        vectordb.persist()
        print(f"ë²¡í„° DBê°€ {VECTOR_DB_PATH}ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë²¡í„° DBì— ì €ì¥í•˜ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        import traceback
        print(traceback.format_exc())

    return final_result


# -------------------
# 2. ë©”ì¸ì—ì„œ í˜¸ì¶œë˜ëŠ” í•µì‹¬ í•¨ìˆ˜: process_urls
# -------------------
def get_video_info(video_url):
    """
    YouTube ì˜ìƒì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. 
    ì˜ìƒ ì œëª©, ì±„ë„ëª… ë“±ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ noembed APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        video_id = video_url.split("v=")[-1].split("&")[0]
        api_url = f"https://noembed.com/embed?url=https://www.youtube.com/watch?v={video_id}"
        response = requests.get(api_url)
        if response.status_code == 200:
            data = response.json()
            return data.get('title'), data.get('author_name')
        return None, None
    except Exception as e:
        print(f"ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None


# -------------------
# 3. ë©”ì¸ -> process_urls -> process_link
# -------------------
def process_link(url):
    """
    ë§í¬ ìœ í˜•ì— ë”°ë¼(ìœ íŠœë¸Œ, í…ìŠ¤íŠ¸ íŒŒì¼, ì›¹í˜ì´ì§€) 
    ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    link_type = detect_link_type(url)
    
    if link_type == "youtube":
        text = get_youtube_transcript(url)
    elif link_type == "text_file":
        text = get_text_from_file(url)
    else:  # ì›¹í˜ì´ì§€
        text = get_text_from_webpage(url)
    
    return text


# -------------------
# 4. process_link -> detect_link_type
# -------------------
def detect_link_type(url):
    """ë§í¬ ìœ í˜• ê°ì§€"""
    if "youtube.com" in url or "youtu.be" in url:
        return "youtube"
    elif url.endswith(".txt"):
        return "text_file"
    elif url.startswith("http"):
        return "webpage"
    else:
        return "unknown"


# -------------------
# 5. process_link -> get_youtube_transcript
# -------------------
def get_youtube_transcript(video_url):
    """
    YouTube ìë§‰ ì¶”ì¶œ ë° íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨.
    í•œêµ­ì–´ ìë§‰ ìš°ì„  -> ì˜ì–´ ìë§‰ -> ê¸°íƒ€ ì–¸ì–´ ìë§‰ ìˆœìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.
    """
    video_id = video_url.split("v=")[-1].split("&")[0]  # ë¹„ë””ì˜¤ ID ì¶”ì¶œ
    try:
        transcripts = YouTubeTranscriptApi.list_transcripts(video_id)
        # ìš°ì„  í•œêµ­ì–´ ìë§‰ ì‹œë„
        if transcripts.find_transcript(['ko']):
            transcript = transcripts.find_transcript(['ko']).fetch()
            transcript_text = "\n".join([f"[{format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript])
            return transcript_text
    except (TranscriptsDisabled, NoTranscriptFound):
        pass
    except Exception as e:
        raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    try:
        # ì˜ì–´ ìë§‰ ì‹œë„
        if transcripts.find_transcript(['en']):
            transcript = transcripts.find_transcript(['en']).fetch()
            transcript_text = "\n".join([f"[{format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript])
            return transcript_text
    except (TranscriptsDisabled, NoTranscriptFound):
        pass
    except Exception as e:
        raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    try:
        # ê¸°íƒ€ ì–¸ì–´ ìë§‰ ì‹œë„
        transcript = transcripts.find_transcript(transcripts._languages).fetch()
        transcript_text = "\n".join([f"[{format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript])
        return transcript_text
    except Exception as e:
        raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")


# -------------------
# 6-1. get_youtube_transcript -> format_timestamp
# -------------------
def format_timestamp(seconds):
    """ì´ˆë¥¼ HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"


# -------------------
# 7. process_link -> get_text_from_file
# -------------------
def get_text_from_file(url):
    """í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš© ì½ê¸°"""
    try:
        response = requests.get(url)
        response.raise_for_status()
        text = response.text.strip()
        return text
    except Exception as e:
        raise ValueError(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# -------------------
# 8. process_link -> get_text_from_webpage
# -------------------
def get_text_from_webpage(url):
    """ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        text = soup.get_text(separator="\n").strip()
        # ê¸¸ì´ ì œí•œ 10000ì
        text = text[:10000]
        return text
    except Exception as e:
        raise ValueError(f"ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# -------------------
# 9. process_urls -> split_text
# -------------------
def split_text(text, max_chunk_size=CHUNK_SIZE):
    """
    í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ í¬ê¸°ì— ë§ê²Œ ë¶„í• í•©ë‹ˆë‹¤.
    ëŒ€ëµì ì¸ ë‹¨ì–´ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• .
    """
    words = text.split()
    total_words = len(words)
    num_chunks = ceil(total_words / (max_chunk_size // 5))
    chunks = []
    for i in range(num_chunks):
        start = i * (max_chunk_size // 5)
        end = start + (max_chunk_size // 5)
        chunk = ' '.join(words[start:end])
        chunks.append(chunk)
    return chunks


# -------------------
# 10. process_urls -> save_chunks
# -------------------
def save_chunks(chunks, directory="chunks"):
    """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not os.path.exists(directory):
        os.makedirs(directory)
    
    for idx, chunk in enumerate(chunks, 1):
        file_path = os.path.join(directory, f"chunk_{idx}.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(chunk)
    print(f"{len(chunks)}ê°œì˜ ì²­í¬ê°€ '{directory}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# -------------------
# 11. process_urls -> summarize_text
# -------------------
def summarize_text(transcript_chunks, model=MODEL):
    """
    ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ChatGPTë¡œ ì„¸ë¶„í™”ëœ ìš”ì•½ ì‘ì—… ìˆ˜í–‰.
    ê° ì²­í¬ë³„ë¡œ ìš”ì•½ì„ ë°›ê³ , ìµœì¢…ì ìœ¼ë¡œ í†µí•© ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    summaries = []
    # (1) ê° ì²­í¬ë¥¼ ìˆœíšŒí•˜ë©° ìš”ì•½
    for idx, chunk in enumerate(transcript_chunks):
        prompt = generate_prompt(chunk)
        try:
            response = openai.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a travel expert who provides detailed recommendations for places to visit, foods to eat, precautions, and suggestions based on transcripts."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            summary = response.choices[0].message.content
            summaries.append(summary)
            print(f"ì²­í¬ {idx+1}/{len(transcript_chunks)} ìš”ì•½ ì™„ë£Œ.")
        except Exception as e:
            raise ValueError(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # (2) ê°œë³„ ìš”ì•½ì„ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summaries = "\n".join(summaries)
    final_prompt = f"""
                ì•„ë˜ëŠ” ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ë‰œ ìš”ì•½ì…ë‹ˆë‹¤. ì´ ìš”ì•½ë“¤ì„ í†µí•©í•˜ì—¬ ë‹¤ìŒì˜ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ê³ , ë¹ ì§€ëŠ” ë‚´ìš© ì—†ì´ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
                **ìš”êµ¬ ì‚¬í•­:**
                1. ì¥ì†Œ, ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ ë“± ê°ê°ì˜ ì •ë³´ë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
                2. ë§Œì•½ í•´ë‹¹ ì¥ì†Œì—ì„œ ë¨¹ì€ ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ì´ ì—†ë‹¤ë©´ ì‘ì„±í•˜ì§€ ì•Šê³  ë„˜ì–´ê°€ë„ ë©ë‹ˆë‹¤.
                3. ë°©ë¬¸í•œ ì¥ì†Œê°€ ì—†ê±°ë‚˜ ìœ ì˜ ì‚¬í•­ë§Œ ìˆì„ ë•Œ, ìœ ì˜ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
                4. ì¶”ì²œ ì‚¬í•­ë§Œ ìˆëŠ” ê²ƒë“¤ì€ ì¶”ì²œ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
                5. ê°€ëŠ¥í•œ ì¥ì†Œ ì´ë¦„ì„ ì•Œê³  ìˆë‹¤ë©´ ì‹¤ì œ ì£¼ì†Œë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.

                ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”
                ì•„ë˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. 

                ë°©ë¬¸í•œ ì¥ì†Œ: ìŠ¤ë¯¸ë‹¤ íƒ€ì›Œ (ì£¼ì†Œ) íƒ€ì„ìŠ¤íƒ¬í”„: [HH:MM:SS]
                - ì¥ì†Œì„¤ëª…: [ìœ íŠœë²„ì˜ ì„¤ëª…] ë„ì¿„ ìŠ¤ì¹´ì´íŠ¸ë¦¬ë¥¼ ëŒ€í‘œí•˜ëŠ” ëœë“œë§ˆí¬ë¡œ, ì „ë§ëŒ€ì—ì„œ ë„ì¿„ ì‹œë‚´ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ íŠœë²„ê°€ ë°©ë¬¸í–ˆì„ ë•ŒëŠ” ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ í›„ì§€ì‚°ê¹Œì§€ ë³´ì˜€ê³ , ì•¼ê²½ì´ íŠ¹íˆ ì•„ë¦„ë‹¤ì› ë‹¤ê³  í•©ë‹ˆë‹¤.
                - ë¨¹ì€ ìŒì‹: ë¼ë©˜ ì´ì¹˜ë€
                    - ì„¤ëª…: ì§„í•œ êµ­ë¬¼ê³¼ ì«„ê¹ƒí•œ ë©´ë°œë¡œ ìœ ëª…í•œ ë¼ë©˜ ì²´ì¸ì ìœ¼ë¡œ, ê°œì¸ì‹¤ì—ì„œ í¸ì•ˆí•˜ê²Œ ì‹ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - ìœ ì˜ ì‚¬í•­: í˜¼ì¡í•œ ì‹œê°„ëŒ€ í”¼í•˜ê¸°
                    - ì„¤ëª…: ê´€ê´‘ì§€ ì£¼ë³€ì€ íŠ¹íˆ ì£¼ë§ê³¼ íœ´ì¼ì— ë§¤ìš° í˜¼ì¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ëŠ¥í•œ í‰ì¼ì´ë‚˜ ì´ë¥¸ ì‹œê°„ì— ë°©ë¬¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
                - ì¶”ì²œ ì‚¬í•­: ìŠ¤ì¹´ì´ íŠ¸ë¦¬ ì „ë§ëŒ€ ë°©ë¬¸
                    - ì„¤ëª…: ë„ì¿„ì˜ ì•„ë¦„ë‹¤ìš´ ì•¼ê²½ì„ ê°ìƒí•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ì§„ ì´¬ì˜ í•˜ê¸°ì— ìµœì ì˜ ì¥ì†Œì…ë‹ˆë‹¤.

                ë°©ë¬¸í•œ ì¥ì†Œ: ìœ ë‹ˆë²„ì…œ ìŠ¤íŠœë””ì˜¤ ì¼ë³¸ (ì£¼ì†Œ) íƒ€ì„ìŠ¤íƒ¬í”„: [HH:MM:SS]
                - ì¥ì†Œì„¤ëª…: [ìœ íŠœë²„ì˜ ì„¤ëª…] ìœ íŠœë²„ê°€ ë°©ë¬¸í–ˆì„ ë•ŒëŠ” í‰ì¼ì„ì—ë„ ì‚¬ëŒì´ ë§ì•˜ì§€ë§Œ, ì‹±ê¸€ë¼ì´ë”ë¥¼ ì´ìš©í•´ì„œ ëŒ€ê¸° ì‹œê°„ì„ ë§ì´ ì¤„ì¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í•´ë¦¬í¬í„° êµ¬ì—­ì˜ ë¶„ìœ„ê¸°ê°€ ì‹¤ì œ ì˜í™”ì˜ í•œ ì¥ë©´ì— ë“¤ì–´ì˜¨ ê²ƒ ê°™ì•˜ê³ , ë²„í„°ë§¥ì£¼ë„ ë§›ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.
                - ìœ ì˜ ì‚¬í•­: ì§§ì€ ì˜· ì°©ìš© 
                    - ì„¤ëª…: íŒ€ë© í”Œë˜ë‹›ì˜ ì¼ë¶€ êµ¬ì—­ì—ì„œëŠ” ë¬¼ì´ ë†’ê³  ê±°ìš¸ì´ ìˆìœ¼ë¯€ë¡œ, ì§§ì€ ì˜·ì„ ì…ëŠ” ê²ƒì´ ì¢‹ë‹¤.

                **ìš”ì•½ ì²­í¬:**
                {combined_summaries}

                **ìµœì¢… ìš”ì•½:**

            """
    try:
        final_response = openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert summary writer who strictly adheres to the provided format."},
                {"role": "user", "content": final_prompt}
            ],
            temperature=0.1,
            max_tokens=4096
        )
        final_summary = final_response.choices[0].message.content
        return final_summary
    except Exception as e:
        raise ValueError(f"ìµœì¢… ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# -------------------
# 11-1. summarize_text -> generate_prompt
# -------------------
def generate_prompt(transcript_chunk):
    """
    ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ OpenAI APIì— ì „ë‹¬.
    í•œêµ­ì–´ê°€ ì•„ë‹ˆë©´ ë²ˆì—­ ì•ˆë‚´ë¥¼ ì¶”ê°€.
    """
    language = detect(transcript_chunk)
    if language != 'ko':
        translation_instruction = "ì´ í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”.\n\n"
    else:
        translation_instruction = ""

    base_prompt = f"""
        {translation_instruction}
        ì•„ë˜ëŠ” ì—¬í–‰ ìœ íŠœë²„ê°€ ì´¬ì˜í•œ ì˜ìƒì˜ ìë§‰ì…ë‹ˆë‹¤. 
        ì´ ìë§‰ì—ì„œ ë°©ë¬¸í•œ ì¥ì†Œ, ë¨¹ì€ ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

        **ìš”êµ¬ ì‚¬í•­:**
        1. ì¥ì†Œ, ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ ë“± ê°ê°ì˜ ì •ë³´ë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        2. ë§Œì•½ í•´ë‹¹ ì¥ì†Œì—ì„œ ë¨¹ì€ ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ì´ ì—†ë‹¤ë©´ ì‘ì„±í•˜ì§€ ì•Šê³  ë„˜ì–´ê°€ë„ ë©ë‹ˆë‹¤.
        3. ë°©ë¬¸í•œ ì¥ì†Œê°€ ì—†ê±°ë‚˜ ìœ ì˜ ì‚¬í•­ë§Œ ìˆì„ ë•Œ, ìœ ì˜ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
        4. ì¶”ì²œ ì‚¬í•­ë§Œ ìˆëŠ” ê²ƒë“¤ì€ ì¶”ì²œ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
        5. ê°€ëŠ¥í•œ ì¥ì†Œ ì´ë¦„ì„ ì•Œê³  ìˆë‹¤ë©´ ì‹¤ì œ ì£¼ì†Œë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
        6. ì¥ì†Œ ì„¤ëª…ì€ ë°˜ë“œì‹œ ìœ íŠœë²„ê°€ ì–¸ê¸‰í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ìœ íŠœë²„ì˜ ì‹¤ì œ ê²½í—˜ê³¼ í‰ê°€ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

        **ê²°ê³¼ í˜•ì‹:**

        ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”
        ì•„ë˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. 

        ë°©ë¬¸í•œ ì¥ì†Œ: ìŠ¤ë¯¸ë‹¤ íƒ€ì›Œ (ì£¼ì†Œ) íƒ€ì„ìŠ¤íƒ¬í”„: [HH:MM:SS]
        - ì¥ì†Œì„¤ëª…: [ìœ íŠœë²„ì˜ ì„¤ëª…] ë„ì¿„ ìŠ¤ì¹´ì´íŠ¸ë¦¬ë¥¼ ëŒ€í‘œí•˜ëŠ” ëœë“œë§ˆí¬ë¡œ, ì „ë§ëŒ€ì—ì„œ ë„ì¿„ ì‹œë‚´ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ íŠœë²„ê°€ ë°©ë¬¸í–ˆì„ ë•ŒëŠ” ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ í›„ì§€ì‚°ê¹Œì§€ ë³´ì˜€ê³ , ì•¼ê²½ì´ íŠ¹íˆ ì•„ë¦„ë‹¤ì› ë‹¤ê³  í•©ë‹ˆë‹¤.
        - ë¨¹ì€ ìŒì‹: ë¼ë©˜ ì´ì¹˜ë€
            - ì„¤ëª…: ì§„í•œ êµ­ë¬¼ê³¼ ì«„ê¹ƒí•œ ë©´ë°œë¡œ ìœ ëª…í•œ ë¼ë©˜ ì²´ì¸ì ìœ¼ë¡œ, ê°œì¸ì‹¤ì—ì„œ í¸ì•ˆí•˜ê²Œ ì‹ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ìœ ì˜ ì‚¬í•­: í˜¼ì¡í•œ ì‹œê°„ëŒ€ í”¼í•˜ê¸°
            - ì„¤ëª…: ê´€ê´‘ì§€ ì£¼ë³€ì€ íŠ¹íˆ ì£¼ë§ê³¼ íœ´ì¼ì— ë§¤ìš° í˜¼ì¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ëŠ¥í•œ í‰ì¼ì´ë‚˜ ì´ë¥¸ ì‹œê°„ì— ë°©ë¬¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        - ì¶”ì²œ ì‚¬í•­: ìŠ¤ì¹´ì´ íŠ¸ë¦¬ ì „ë§ëŒ€ ë°©ë¬¸
            - ì„¤ëª…: ë„ì¿„ì˜ ì•„ë¦„ë‹¤ìš´ ì•¼ê²½ì„ ê°ìƒí•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ì§„ ì´¬ì˜ í•˜ê¸°ì— ìµœì ì˜ ì¥ì†Œì…ë‹ˆë‹¤.

        ë°©ë¬¸í•œ ì¥ì†Œ: ìœ ë‹ˆë²„ì…œ ìŠ¤íŠœë””ì˜¤ ì¼ë³¸ (ì£¼ì†Œ) íƒ€ì„ìŠ¤íƒ¬í”„: [HH:MM:SS]
        - ì¥ì†Œì„¤ëª…: [ìœ íŠœë²„ì˜ ì„¤ëª…] ìœ íŠœë²„ê°€ ë°©ë¬¸í–ˆì„ ë•ŒëŠ” í‰ì¼ì„ì—ë„ ì‚¬ëŒì´ ë§ì•˜ì§€ë§Œ, ì‹±ê¸€ë¼ì´ë”ë¥¼ ì´ìš©í•´ì„œ ëŒ€ê¸° ì‹œê°„ì„ ë§ì´ ì¤„ì¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í•´ë¦¬í¬í„° êµ¬ì—­ì˜ ë¶„ìœ„ê¸°ê°€ ì‹¤ì œ ì˜í™”ì˜ í•œ ì¥ë©´ì— ë“¤ì–´ì˜¨ ê²ƒ ê°™ì•˜ê³ , ë²„í„°ë§¥ì£¼ë„ ë§›ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.
        - ìœ ì˜ ì‚¬í•­: ì§§ì€ ì˜· ì°©ìš© 
            - ì„¤ëª…: íŒ€ë© í”Œë˜ë‹›ì˜ ì¼ë¶€ êµ¬ì—­ì—ì„œëŠ” ë¬¼ì´ ë†’ê³  ê±°ìš¸ì´ ìˆìœ¼ë¯€ë¡œ, ì§§ì€ ì˜·ì„ ì…ëŠ” ê²ƒì´ ì¢‹ë‹¤.

        **ìë§‰:**
        {transcript_chunk}

        ì´ ìë§‰ì„ ë°”íƒ•ìœ¼ë¡œ ìœ„ì˜ ìš”êµ¬ ì‚¬í•­ì— ë§ëŠ” ì •ë³´ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”. íŠ¹íˆ ì¥ì†Œ ì„¤ëª…ì€ ë°˜ë“œì‹œ ìœ íŠœë²„ê°€ ì‹¤ì œë¡œ ì–¸ê¸‰í•œ ë‚´ìš©ê³¼ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        """
    return base_prompt


# -------------------
# 12. process_urls -> extract_place_names
# -------------------
def extract_place_names(summary):
    """ìš”ì•½ì—ì„œ 'ë°©ë¬¸í•œ ì¥ì†Œ:' ë¼ì¸ì„ ì°¾ì•„ ì¥ì†Œ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    place_names = []
    lines = summary.split("\n")
    
    for line in lines:
        if line.startswith("ë°©ë¬¸í•œ ì¥ì†Œ:"):
            try:
                place_info = line.replace("ë°©ë¬¸í•œ ì¥ì†Œ:", "").strip()
                place_name = place_info.split("(")[0].strip()
                if place_name and place_name not in place_names:
                    place_names.append(place_name)
            except Exception as e:
                print(f"ì¥ì†Œ ì´ë¦„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
    
    return place_names


# -------------------
# 13. process_urls -> search_place_details
# -------------------
def search_place_details(place_name):
    """
    Google Places APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    - ê°€ì¥ í‰ì ì´ ë†’ì€(í˜¹ì€ 5ì  ë¦¬ë·°ê°€ ìˆë‹¤ë©´ ê°€ì¥ ê¸´) ë¦¬ë·°ë¥¼ 'best_review'ë¡œ ì„¤ì •
    - ì‚¬ì§„, ì—°ë½ì²˜, ì˜ì—…ì‹œê°„, ì›¹ì‚¬ì´íŠ¸ ë“± ì •ë³´ë¥¼ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        # ì¥ì†Œ ê²€ìƒ‰ (ì¼ë³¸ ì§€ì—­ ìš°ì„ )
        search_url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
        search_params = {
            "query": f"{place_name} ",
            "key": GOOGLE_PLACES_API_KEY,
            "language": "ko",
            "region": "jp"
        }
        
        response = requests.get(search_url, params=search_params)
        data = response.json()
        
        if data.get('results'):
            place = data['results'][0]
            details = {
                'name': place.get('name'),
                'formatted_address': place.get('formatted_address'),
                'rating': place.get('rating'),
                'place_id': place.get('place_id')
            }
            
            # ì¥ì†Œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            details_url = "https://maps.googleapis.com/maps/api/place/details/json"
            details_params = {
                "place_id": details['place_id'],
                "fields": "formatted_phone_number,website,opening_hours,price_level,reviews,photos,editorial_summary,price_level,current_opening_hours",
                "key": GOOGLE_PLACES_API_KEY,
                "language": "ko",
                "reviews_sort": "rating"
            }
            
            details_response = requests.get(details_url, params=details_params)
            details_data = details_response.json()
            
            if 'result' in details_data:
                result = details_data['result']
                
                # í‰ì ì´ ê°€ì¥ ë†’ì€ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
                best_review = None
                if 'reviews' in result:
                    reviews = result['reviews']
                    five_star_reviews = [r for r in reviews if r.get('rating', 0) == 5]
                    if five_star_reviews:
                        best_review = max(five_star_reviews, key=lambda x: len(x.get('text', '')))
                    else:
                        best_review = max(reviews, key=lambda x: (x.get('rating', 0), len(x.get('text', ''))))
                
                details.update({
                    'phone': result.get('formatted_phone_number'),
                    'website': result.get('website'),
                    'opening_hours': result.get('opening_hours', {}).get('weekday_text'),
                    'current_opening_hours': result.get('current_opening_hours', {}).get('weekday_text'),
                    'price_level': result.get('price_level'),
                    'best_review': best_review,
                    'photos': result.get('photos', [])[:5],
                    'editorial_summary': result.get('editorial_summary', {}).get('overview')
                })
            return details
        else:
            print(f"ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {place_name}")
            return None
    except Exception as e:
        print(f"ì¥ì†Œ ìƒì„¸ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# -------------------
# 14. process_urls -> get_place_photo_google
# -------------------
def get_place_photo_google(place_name, api_key):
    """
    Google Places APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œ IDë¥¼ ê²€ìƒ‰í•œ ë’¤,
    place_idë¡œ ì‚¬ì§„ì˜ photoreferenceë¥¼ ì–»ê³ 
    ìµœì¢…ì ìœ¼ë¡œ ì‚¬ì§„ URLì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    search_url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
    search_params = {
        "input": place_name,
        "inputtype": "textquery",
        "fields": "photos,place_id",
        "key": api_key
    }
    search_response = requests.get(search_url, params=search_params)
    if search_response.status_code == 200:
        search_data = search_response.json()
        if search_data['candidates']:
            place_id = search_data['candidates'][0]['place_id']
            details_url = "https://maps.googleapis.com/maps/api/place/details/json"
            details_params = {
                "place_id": place_id,
                "fields": "photos",
                "key": api_key
            }
            details_response = requests.get(details_url, params=details_params)
            if details_response.status_code == 200:
                details_data = details_response.json()
                if 'photos' in details_data['result']:
                    photo_reference = details_data['result']['photos'][0]['photo_reference']
                    photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_reference}&key={api_key}"
                    return photo_url
        return "ì‚¬ì§„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    else:
        return "API ìš”ì²­ ì‹¤íŒ¨."


# -------------------
# 15. process_urls -> save_final_summary
# -------------------
def save_final_summary(final_summary):
    os.makedirs(SUMMARY_DIR, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    file_path = os.path.join(SUMMARY_DIR, f"final_summary_{timestamp}.txt")
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(final_summary)
        print(f"ìµœì¢… ìš”ì•½ì´ '{file_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ìµœì¢… ìš”ì•½ì„ ì €ì¥í•˜ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# -------------------
# (ê¸°íƒ€) ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìœ¼ë‚˜ ì›ë³¸ ì½”ë“œì— í¬í•¨ëœ í•¨ìˆ˜ë“¤
# -------------------
def count_tokens(text, model=MODEL):
    """
    í…ìŠ¤íŠ¸ê°€ ëª‡ ê°œì˜ í† í°ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ì§€ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    ì›ë³¸ ì½”ë“œì— í¬í•¨ë˜ì–´ ìˆìœ¼ë‚˜ ì‹¤ì œë¡œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))


def get_address_google(place_name, api_key):
    """
    Google Geocoding APIë¥¼ í†µí•´ ì£¼ì†Œë¥¼ ì°¾ëŠ” í•¨ìˆ˜.
    ì›ë³¸ ì½”ë“œì— í¬í•¨ë˜ì–´ ìˆìœ¼ë‚˜ ì‹¤ì œë¡œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"
    params = {
        "address": place_name,
        "key": api_key
    }
    response = requests.get(base_url, params=params)
    if response.status_code == 200:
        data = response.json()
        if data['results']:
            return data['results'][0]['formatted_address']
        else:
            return "ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    else:
        return "API ìš”ì²­ ì‹¤íŒ¨."


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œë¥¼ í•¨ìˆ˜ ì •ì˜ ë’¤ë¡œ ì´ë™
if __name__ == "__main__":
    print("ìµœëŒ€ 5ê°œì˜ URLì„ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ ë¹ˆ ì¤„ì„ ì…ë ¥í•˜ì„¸ìš”.")
    input_urls = []
    for i in range(MAX_URLS):
        url = input(f"URL {i+1}: ").strip()
        if not url:
            break
        input_urls.append(url)
    
    if not input_urls:
        print("ì…ë ¥ëœ URLì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        try:
            summary = process_urls(input_urls)
            print("\n[ìµœì¢… ìš”ì•½]")
            print(summary)
            
            print("\në²¡í„° DB í™•ì¸ì„ ìœ„í•´ run_vectordb.pyë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            run_vectordb_path = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                'ai_vector',
                'run_vectordb.py'
            )
            os.system(f"python3 {run_vectordb_path}")
        except Exception as e:
            print(f"ì˜¤ë¥˜: {e}")
